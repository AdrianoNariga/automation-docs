Documentação de referencia:
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Reference/ch-resourceconstraints-HAAR.html
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Reference/pacemaker_remote.html
http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html-single/Pacemaker_Remote/


Este tutorial foi feito usando 4 máquinas, 2 para o cluster e 2 para os nós remotos.

A instalação foi dividida em duas partes:
1 - instalacao dos nós remotos (rodando pacemaker_remote)
2 - instalação do cluster (todos os serviços relacionados ao cluster - corosync, pacemaker, etc)

Apos instalação os nós remotos serão adicionados ao cluster.


Em todos os nós desabilitar o SELINUX e limpar as regras de firewall:

setenforce 0
sed -i.bak "s/SELINUX=enforcing/SELINUX=permissive/g" /etc/selinux/config
systemctl disable firewalld.service
systemctl stop firewalld.service
iptables --flush


1 - Instalaçao dos nós remotos:

1.1 - instalar os pacotes necessários para gerenciamento
yum install -y pacemaker-remote resource-agents pcs

1.2 - Criar e editar a permissão do diretório usado pelo pacemaker
mkdir -p --mode=0750 /etc/pacemaker
chgrp haclient /etc/pacemaker

1.3 - Criar uma chave de autenticacao (esta chave deve estar presente em TODOS os nós)
dd if=/dev/urandom of=/etc/pacemaker/authkey bs=4096 count=1

1.4 - iniciar e habilitar o serviço pacemaker_remote
systemctl enable pacemaker_remote.service
systemctl start pacemaker_remote.service

1.5 - Verificar se o serviço está rodando - por default irá rodar na porta 3121, para alterar esta porta editar o arquivo /etc/sysconfig/pacemaker

1.6 - Testar a conectividade entre o cluster e os nós remotos
ssh -p 3121 ip_ou_nome_host_remoto (executar o comando a partir dos nós do cluster)
Se o resultado for "exchange_identification: read: Connection reset by peer" a conectividade está ok,
Se o resultado for "ssh: connect to host remote1 port 3121: No route to host" verificar a conectividade entre os hosts.


2 - Instalação do cluster

2.1 - adicionar ao arquivo de hosts ou no dns o nome-ip dos nós remotos

2.2 - Instalar o software do cluster, criar e editar a permissão do diretório usado pelo pacemaker
yum install -y pacemaker corosync pcs resource-agents
mkdir -p --mode=0750 /etc/pacemaker
chgrp haclient /etc/pacemaker

2.3 - Copiar a chave criada no passo 1.3 para dentro do diretório /etc/pacemaker

2.4 - criar senha para o usuario hacluster em ambos nós do cluster

2.5 - iniciar e habilitar o serviço pcsd
systemctl start pcsd.service
systemctl enable pcsd.service

2.6 - autenticar os hosts
pcs cluster auth nome_do_cluster1 nome_do_cluster2 - será solicitado usuario e senha, usar o mesmo do passo 2.4

2.7 - Criar o arquivo corosync.conf - o arquivo sera copiado para o outro nó automaticamente
pcs cluster setup --name nome_do_cluster <node1 ip or hostname> <node2 ip or hostname>

2.8 - iniciar o cluster
pcs cluster start --all

2.9 - após alguns minutos conferir se os 2 nós estão online
pcs status

2.10 - desabilitar o stonith (temporario)
pcs property set stonith-enabled=false


3 - Integração dos nós remotos no cluster

3.1 - Adicionando os nós remotos
pcs resource create remote1 ocf:pacemaker:remote
pcs resource create remote2 ocf:pacemaker:remote


3.2 - Evitando que o recurso vá para os nós do cluster (caso seja necessário que os mesmos rodem somente no

3.2.1 - Setar a propriedade symmetric-cluster para true
pcs property set symmetric-cluster=true - está opção permite que os recursos rodem em qualquer nó do cluster

3.2.2 - Criando o recurso - neste exemplo está sendo usado o httpd
pcs resource create webserver ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf op monitor interval=20s

3.2.3 - Evitando que o recurso vá para um nó especifico do cluster - neste caso o recurso deve rodar somente nos nós remotos
pcs constraint location webserver avoids node1
pcs constraint location webserver avoids node2
